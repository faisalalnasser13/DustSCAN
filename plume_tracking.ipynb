{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d2bc86-25c1-4143-afc4-73cd8f94d149",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e6e47-8ddb-42af-8783-69643f7fdc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from random import random\n",
    "import xarray as xr\n",
    "import xarray as xr\n",
    "from satpy import Scene\n",
    "from pyresample.geometry import AreaDefinition\n",
    "from pyresample.geometry import GridDefinition\n",
    "from pyresample.kd_tree import resample_nearest\n",
    "from sklearn.cluster import DBSCAN\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce0db16-8b1a-4f54-b658-6bed0266cd19",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd1943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_globe_area(fraction):\n",
    "    \"\"\"\n",
    "    Generate an AreaDefinition object for a global grid based on the specified fraction.\n",
    "\n",
    "    This function creates an AreaDefinition object representing a global grid,\n",
    "    where the grid's resolution is determined by the given fraction. The function\n",
    "    utilizes the pyresample library to create this definition. The grid covers\n",
    "    the entire globe, with longitude ranging from -180 to 180 degrees and latitude\n",
    "    from -90 to 90 degrees.\n",
    "\n",
    "    Parameters:\n",
    "    fraction (float): The fraction of a degree to define the grid resolution. For example,\n",
    "                      a fraction of 0.5 would mean each grid cell is 0.5 degrees by 0.5 degrees.\n",
    "\n",
    "    Returns:\n",
    "    AreaDefinition: A pyresample AreaDefinition object representing the global grid.\n",
    "\n",
    "    Note:\n",
    "    - The projection used is EPSG:4326, which is the standard geographic coordinate system.\n",
    "    - The grid width and height are calculated as multiples of 360 and 180 degrees, respectively,\n",
    "      based on the inverse of the fraction provided.\n",
    "    - The AreaDefinition object contains attributes like area_id, description, proj_id, projection,\n",
    "      width, height, and area_extent which are configured for the global grid.\n",
    "    \"\"\"\n",
    "    constant = 1/fraction\n",
    "    area_id = 'globe'\n",
    "    description = 'globe'\n",
    "    proj_id = 'globe'\n",
    "    projection = \"EPSG:4326\"  # Use 'EPSG:3409' with pyproj 2.0+\n",
    "    width = 360*constant\n",
    "    height = 180*constant\n",
    "    area_extent = (-180,-90, 180,90)\n",
    "    area_def = AreaDefinition(area_id, description, proj_id, projection,\n",
    "                              width, height, area_extent)\n",
    "    return area_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e31329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampler(lons, lats, data ,fraction_of_degree):\n",
    "    \"\"\"\n",
    "    Resample spatial data to a new grid defined by a specific fraction of a degree.\n",
    "\n",
    "    This function takes longitude and latitude arrays, along with associated data,\n",
    "    and resamples it onto a new grid defined by `fraction_of_degree`. The new grid\n",
    "    is defined over a global area. Nearest-neighbor resampling is used with a\n",
    "    specified radius of influence.\n",
    "\n",
    "    Parameters:\n",
    "    lons (array-like): Array of longitudes corresponding to the data points.\n",
    "    lats (array-like): Array of latitudes corresponding to the data points.\n",
    "    data (array-like): The data to be resampled, corresponding to each (lon, lat) pair.\n",
    "    fraction_of_degree (float): The grid size for the new grid, specified as a fraction of a degree.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: The resampled data on the new grid.\n",
    "\n",
    "    Note:\n",
    "    This function relies on predefined functions `get_globe_area` for defining the\n",
    "    target grid area and `resample_nearest` from a spatial resampling library to\n",
    "    perform the nearest-neighbor resampling.\n",
    "    \"\"\"\n",
    "    area_def = get_globe_area(fraction_of_degree)\n",
    "    grid_def = GridDefinition(lons=lons, lats=lats)\n",
    "    result = resample_nearest(grid_def, data , area_def, radius_of_influence=15000, fill_value=None)\n",
    "    return result.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301285f2-4d9d-4482-89a8-3cecafb0a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lon_lat(file):\n",
    "    \"\"\"\n",
    "    Reads longitude and latitude data from a given satellite data file using the SEVIRI L1b native reader. This function\n",
    "    loads a specific channel from the satellite data to extract the geographic information and then returns subsets of\n",
    "    longitude and latitude arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        File path of the satellite data file to be read.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lons, lats : tuple of numpy.ndarray\n",
    "        Two 2D numpy arrays containing subsets of longitude and latitude values, respectively. The subsets are\n",
    "        specific portions of the full arrays, tailored to the region of interest.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function uses the `Scene` class from the satellite data processing library to read the specified file.\n",
    "    It specifically loads the 'VIS006' channel (visible light) to access the geographical information. The function\n",
    "    then extracts the required longitude and latitude data from this channel. The indices for subsetting (1800:3200,\n",
    "    400:3700) are hardcoded and may need adjustment based on the region of interest or the specifics of the satellite data.\n",
    "    \"\"\"\n",
    "    scn = Scene(reader=\"seviri_l1b_native\", filenames=[file])\n",
    "    chanels = scn.all_dataset_names()\n",
    "    chanels=chanels[1:]\n",
    "    scn.load([\"VIS006\"])\n",
    "    lons, lats = scn[\"VIS006\"].attrs['area'].get_lonlats()\n",
    "    return lons[1800:3200,400:3700], lats[1800:3200,400:3700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf3a1e6-bfa9-43b9-8415-24173b20412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dust_rgb(filename , lons,lats):\n",
    "    \"\"\"\n",
    "    Processes a satellite data file to create a dust RGB image. This function loads specific infrared channels, \n",
    "    applies a formula to generate a dust RGB image, and resamples it to a global grid using provided longitude \n",
    "    and latitude arrays. The RGB channels are adjusted to be within the 0-1 range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        File path of the satellite data file.\n",
    "    lons : numpy.ndarray\n",
    "        2D array of longitude values for spatial referencing in resampling.\n",
    "    lats : numpy.ndarray\n",
    "        2D array of latitude values for spatial referencing in resampling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    img : numpy.ndarray\n",
    "        A 3D numpy array representing the resampled dust RGB image. Values are normalized to the 0-1 range.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function uses the 'IR_087', 'IR_108', and 'IR_120' channels from the SEVIRI L1b native reader. It \n",
    "    computes the red, green, and blue channels using specific formulas and ranges for each channel. The resultant \n",
    "    image is then resampled to align with the provided longitude and latitude arrays. Post-resampling, the image \n",
    "    is further processed to ensure all values are within the 0-1 range, clipping any outliers.\n",
    "    \"\"\"\n",
    "    #print(filename)\n",
    "    scn = Scene(reader=\"seviri_l1b_native\", filenames=[filename])\n",
    "    chanels = scn.all_dataset_names()\n",
    "    chanels=chanels[1:]\n",
    "    scn.load(['IR_087',\"IR_108\", 'IR_120'])\n",
    "\n",
    "    #\n",
    "    rmin = -4\n",
    "    rmax = 2\n",
    "    gmin =0\n",
    "    gmax=15\n",
    "    bmin = 261\n",
    "    bmax = 289\n",
    "\n",
    "    c087= scn[\"IR_087\"][1800:3200,400:3700].values[:,:,np.newaxis]\n",
    "    c108= scn[\"IR_108\"][1800:3200,400:3700].values[:,:,np.newaxis]\n",
    "    c120= scn['IR_120'][1800:3200,400:3700].values[:,:,np.newaxis]\n",
    "\n",
    "\n",
    "    r = ((c120-c108)-rmin)/(rmax-rmin)\n",
    "    g = ((c108-c087)-gmin)/(gmax-gmin)\n",
    "    g = g ** (1/2.5)\n",
    "    b = (c108-bmin)/(bmax-bmin)\n",
    "    img = np.concatenate([r,g,b], axis=2)\n",
    "    img[img > 1] = 1\n",
    "    img[img < 0] = 0\n",
    "    img[np.isnan(img)] = 0\n",
    "    #\n",
    "    y0,y1,x0,x1 =190,338,673,1030\n",
    "\n",
    "    #\n",
    "    resampled = resampler(lons,lats,img , 0.25)\n",
    "    img = resampled[y0:y1,x0:x1]\n",
    "    img[img>100]=0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe917867-c810-402f-9ee1-160ac29df435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cloud(file, lons, lats):\n",
    "    \"\"\"\n",
    "    Reads cloud mask data from a .grb file and resamples it to match a given Earth grid defined by longitude and latitude arrays.\n",
    "\n",
    "    This function opens a .grb file containing cloud mask data, where sea is represented by 0, land by 1, and clouds by 2. It then resamples this data to fit the provided longitude and latitude grid. The function applies specific slicing to the cloud data and uses a custom resampler to fit the data into the Earth grid. Values greater than 10 in the resampled cloud data are set to 2, indicating cloud presence.\n",
    "\n",
    "    Parameters:\n",
    "    file (str): Path to the .grb file containing cloud mask data.\n",
    "    lons (numpy.ndarray): Array of longitudes for resampling the cloud data.\n",
    "    lats (numpy.ndarray): Array of latitudes for resampling the cloud data.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Array representing the resampled cloud mask data.\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(file ,engine='cfgrib')\n",
    "    clouds = ds[\"p260537\"]\n",
    "    clouds = np.reshape(clouds.values, (3712, 3712), order='C')[1800:3200,400:3700]\n",
    "    y0,y1,x0,x1 =190,338,673,1030\n",
    "    clouds = resampler(lons,lats,clouds , 0.25)\n",
    "    clouds = clouds[y0:y1,x0:x1]\n",
    "    clouds[clouds>10] =2\n",
    "    \n",
    "    return clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pink_channel(pink_distance):\n",
    "    \"\"\"\n",
    "    Normalizes the pink distance values to a scale of 0-1, with higher values indicating greater pinkness. This function\n",
    "    inverts and scales the provided pink distance values so that a higher value corresponds to a color closer to the\n",
    "    reference pink.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pink_distance : numpy.ndarray\n",
    "        A 2D numpy array containing distances of image pixels from the reference pink color.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pink_c : numpy.ndarray\n",
    "        A 2D numpy array where each value represents the normalized pinkness of the corresponding pixel, ranging from 0 to 1.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function uses a predefined maximum distance (1.732051) to normalize the input distances. The normalization\n",
    "    process inverts the distances (i.e., subtracts them from the maximum distance) and then divides by the maximum\n",
    "    distance, effectively rescaling them to a 0-1 range. A value of 1 indicates a color identical to the reference\n",
    "    pink, while a value of 0 indicates the furthest possible color from pink in the RGB color space.\n",
    "    \"\"\"\n",
    "    max_distance = 1.732051\n",
    "    pink_c = (- pink_distance + max_distance )/max_distance\n",
    "    return pink_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a02c35-6a71-4e06-bf3c-79acd3f2d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_PDI(filename, lons, lats):\n",
    "    \"\"\"\n",
    "    Processes a dust RGB image to extract and normalize pink channel data. This function reads a dust RGB image,\n",
    "    calculates the distance of each pixel's color from a reference pink color, and normalizes these distances\n",
    "    to a 0-1 scale.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        File path of the dust RGB image.\n",
    "    lons : numpy.ndarray\n",
    "        Array of longitude values for spatial referencing in the image.\n",
    "    lats : numpy.ndarray\n",
    "        Array of latitude values for spatial referencing in the image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PDI : numpy.ndarray\n",
    "        A 2D numpy array representing the normalized pink channel data extracted from the dust RGB image.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function first reads the dust RGB image using provided longitude and latitude values for resampling.\n",
    "    It then computes the distance of each pixel color from a predefined reference pink color ([1, 0, 1]).\n",
    "    These distances are normalized to a range of 0-1, representing the intensity of the pink channel in the image,\n",
    "    which is indicative of dust presence.\n",
    "    \"\"\"\n",
    "    \n",
    "    ''' takes dust rgb, gives pink distances (normalized to 0-1 using get_pink_channel), passes lons and lats to read dust RGB for resampling'''\n",
    "    dust_img = read_dust_rgb(filename, lons, lats)\n",
    "    ref_pink = np.array([1 , 0 ,1 ])\n",
    "    ref_pink = ref_pink[np.newaxis,np.newaxis,:]\n",
    "    #\n",
    "    subtracted = dust_img-ref_pink\n",
    "    dist = np.linalg.norm(subtracted, axis=2)\n",
    "    PDI = get_pink_channel(dist)[:,:,np.newaxis]\n",
    "    return PDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115438d9-2834-4dd0-b5f9-6a7225139d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_aligner(all_files, cloud_files,seviri_dirname, cloud_dirname):\n",
    "    \"\"\"\n",
    "    Aligns and pairs files from two different datasets based on their dates. This function processes dictionaries\n",
    "    containing file paths from two datasets (Seviri L1.5 and corresponding cloud masks), aligns them by their dates, and\n",
    "    ensures that each file from one dataset has a corresponding file from the other dataset with the same date.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_files : dict\n",
    "        A dictionary where each key represents month and hour (MM-hh), and each value\n",
    "        is a list of file paths for that month and hour.\n",
    "    cloud_files : dict\n",
    "        A dictionary similar to `all_files`, but containing file paths for cloud mask data.\n",
    "    seviri_dirname: str\n",
    "        Directory containing seviri files\n",
    "    cloud_dirname: str\n",
    "        Directory containing seviri cloud mask files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_files, cloud_files : tuple of dicts\n",
    "        Two dictionaries with aligned file paths. The keys remain the same, but the lists of file paths are filtered\n",
    "        so that each file in `all_files` has a corresponding file in `cloud_files` with the same date.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function extracts date information from the file names, assumes a specific format for the file names, and\n",
    "    uses this to align the files. It is important that the file naming conventions are consistent and include date\n",
    "    information for the function to work correctly. The function uses pandas DataFrames to facilitate the alignment\n",
    "    process.\n",
    "    \"\"\"\n",
    "    keys = list(all_files.keys())\n",
    "    for key in keys:      \n",
    "        l= all_files[key]\n",
    "        dates_sev = [i[24+len(seviri_dirname):34+len(seviri_dirname)] for i in l]\n",
    "        df = pd.DataFrame()\n",
    "        df[\"dates\"] = dates_sev\n",
    "        df[\"index_seviri\"] = np.arange(len(dates_sev))\n",
    " \n",
    "        l= cloud_files[key]\n",
    "        dates_cloud = [i[28+len(cloud_dirname):38+len(cloud_dirname)] for i in l]\n",
    "        df2 = pd.DataFrame()\n",
    "        df2[\"dates\"] = dates_cloud\n",
    "        df2[\"index_cloud\"] = np.arange(len(dates_cloud))\n",
    " \n",
    "        merged = pd.merge(df,df2, on=\"dates\" , how = \"inner\" )\n",
    " \n",
    "        seviri_index= merged[\"index_seviri\"].values\n",
    "        cloud_index = merged [\"index_cloud\"].values\n",
    "        #\n",
    "        sev = all_files[key]\n",
    "        sev = [sev[i] for i in seviri_index]\n",
    "        all_files[key] = sev\n",
    "        #\n",
    "        cloud = cloud_files[key]\n",
    "        cloud = [cloud[i] for i in cloud_index]\n",
    "        cloud_files[key] = cloud\n",
    "    return all_files, cloud_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfa1f69-6528-4aad-8287-212d7a19338b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_reference_PDI(files_list, cloud_files_list , lons , lats ):\n",
    " \n",
    "    \"\"\"\n",
    "    Aggregates pink array data from multiple seviri files, adjusting for cloud coverage. This function reads seviri L1.5 and cloud mask\n",
    "    data from corresponding file lists, processes them to handle cloud coverage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files_list : list of str\n",
    "        List of file paths containing pink array data.\n",
    "    cloud_files_list : list of str\n",
    "        List of file paths containing cloud coverage data.\n",
    "    lons : numpy.ndarray\n",
    "        Array of longitude values for spatial referencing.\n",
    "    lats : numpy.ndarray\n",
    "        Array of latitude values for spatial referencing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    final : numpy.ndarray\n",
    "        A 2D numpy array representing the aggregated and adjusted pink data. Areas with insufficient data due to\n",
    "        cloud coverage are set to NaN.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function processes each file pair (pink and cloud) by applying cloud masks to the pink data. Pink values\n",
    "    are set to NaN where cloud coverage is significant. The function then accumulates this data across all files,\n",
    "    calculating a median value for each spatial location, while considering cloud coverage. Locations with\n",
    "    limited data (as defined by a threshold) due to cloud coverage are marked as NaN in the final output.\n",
    "    \"\"\"    \n",
    "    j=0\n",
    "    for i in range(len(files_list)):\n",
    "        pink = read_PDI(files_list[i] ,lons, lats)\n",
    "        cloud = read_cloud(cloud_files_list[i] , lons, lats)\n",
    "        pink[cloud==2] = np.nan\n",
    "        counter = cloud.copy()[:,:,np.newaxis]\n",
    "        counter[counter!=2] = 1\n",
    "        counter[counter==2]= 0\n",
    "        if j == 0:\n",
    "            reference_pink = pink.copy()\n",
    "            temp_counter = counter.copy()\n",
    "        else:\n",
    "            reference_pink = np.concatenate([reference_pink , pink] , axis = 2)\n",
    "            temp_counter = np.concatenate([temp_counter , counter] , axis = 2)\n",
    "        j=j+1\n",
    "        if j == len(files_list):\n",
    "            last_counter = np.nansum(temp_counter, axis =2)[:,:,np.newaxis]\n",
    "    final =np.nanmedian(reference_pink,axis=2)\n",
    "    mask = last_counter<=5\n",
    "    final[mask[:,:,0]]=np.nan\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7b1021-0adb-4220-9319-809edcd4bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PDI_anomaly(PDI , date, PDI_monthly_hourly_median ):\n",
    "    \"\"\"\n",
    "    Calculate the Pink Dust Index (PDI) anomaly for a given date and time.\n",
    "\n",
    "    This function computes the PDI anomaly by subtracting the median PDI value \n",
    "    for the corresponding month and hour from the provided PDI array.\n",
    "\n",
    "    Parameters:\n",
    "    - PDI (array): The Pink Dust Index array for which the anomaly is to be calculated.\n",
    "    - PDI_monthly_hourly_median (dict): A dictionary containing median PDI values \n",
    "      indexed by month and hour keys.\n",
    "    - date (str): The date and time in the format 'YYYY-MM-DD' used to \n",
    "      extract the month and hour for indexing the median PDI values.\n",
    "\n",
    "    Returns:\n",
    "    - PDI_anomaly (array): The computed PDI anomaly array\n",
    "    \n",
    "    \"\"\"\n",
    "    month = date[5:7]\n",
    "    hour = date[8:10]\n",
    "    key = month + \"-\" + hour\n",
    "    #\n",
    "    PDI_monthly_hourly_median = PDI_monthly_hourly_median[key][:]\n",
    "    PDI_monthly_hourly_median = PDI_monthly_hourly_median [:,:,np.newaxis] \n",
    "    PDI_anomaly = (PDI - PDI_monthly_hourly_median)\n",
    "    PDI_anomaly[PDI_anomaly<0] = 0\n",
    "    PDI_anomaly[np.isnan(PDI_anomaly)] = 0\n",
    "    return PDI_anomaly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2fdbdb-425a-4ca3-b65e-aa4fa1828236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_cloud_mask(filename, lons, lats):\n",
    "    \"\"\"\n",
    "    Reads and processes a satellite data file to generate a cloud mask. The function loads an infrared channel,\n",
    "    uses it to create a preliminary cloud mask based on a specific temperature threshold, and then resamples this \n",
    "    mask to align with provided longitude and latitude arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        File path of the satellite data file.\n",
    "    lons : numpy.ndarray\n",
    "        2D array of longitude values for spatial referencing in resampling.\n",
    "    lats : numpy.ndarray\n",
    "        2D array of latitude values for spatial referencing in resampling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mask : numpy.ndarray\n",
    "        A 3D numpy array (with a singleton third dimension) representing the resampled cloud mask. Each element \n",
    "        indicates the presence (1) or absence (0) of clouds.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The cloud mask is derived from the 'IR_108' channel of the SEVIRI L1b native reader. A temperature threshold \n",
    "    (less than 272K) is used to identify cloud-covered areas. The initial mask is then resampled to match the \n",
    "    provided spatial grid and further processed to ensure binary values (0 or 1). The final mask is cropped to \n",
    "    a specific region defined by hardcoded indices, which may need to be adjusted based on the region of interest.\n",
    "    \"\"\"\n",
    "    scn = Scene(reader=\"seviri_l1b_native\", filenames=[filename])\n",
    "    scn.load([\"IR_108\"])\n",
    "    mask = scn[\"IR_108\"][1800:3200,400:3700].values <272\n",
    "    mask = resampler(lons,lats , mask.astype(int) , 0.25)\n",
    "    #\n",
    "    y0,y1,x0,x1 =190,338,673,1030\n",
    "\n",
    "\n",
    "    #\n",
    "    mask = mask[y0:y1,x0:x1]\n",
    "    mask[mask>10] = 1\n",
    "    mask = mask==1\n",
    "    return mask[:,:,np.newaxis] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ee90c-0371-4fff-bb44-abb01d11eab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transformer(x , thresh=0.08):\n",
    "    \"\"\"\n",
    "    Transforms the values in an array by scaling by 10 and replacing small and NaN values with -15.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): The input array to be transformed.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: The transformed array.\n",
    "    \"\"\"\n",
    "    s = (x*10)\n",
    "    s[x<thresh]=-15\n",
    "    s[np.isnan(s)]=-15\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd1748-f7e8-437b-b75f-7964eb707c85",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15701c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seviri_dirname = # this should be a directory that contains seviri Level 1.5 file in .nat format, the files should be from the same year\n",
    "seviri_files={}\n",
    "for files in os.listdir(seviri_dirname):\n",
    "    if files.endswith(\".nat\"):\n",
    "        month = files[28:30]\n",
    "        day = files[30:32]\n",
    "        hour = files[32:34]\n",
    "        key = month+\"-\"+hour\n",
    "        if key in seviri_files:\n",
    "            seviri_files[key].append(seviri_dirname+files)\n",
    "        else:\n",
    "            seviri_files[key]= [seviri_dirname+files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_dirname = # this should be a directory that contains seviri cloud masks in .grb format, the files should be from the same year \n",
    "cloud_files={}\n",
    "for files in os.listdir(cloud_dirname):\n",
    "    if files.endswith(\".grb\"):\n",
    "        month = files[32:34]\n",
    "        day = files[34:36]\n",
    "        hour = files[36:38]\n",
    "        key = month+\"-\"+hour\n",
    "        if key in cloud_files:\n",
    "            cloud_files[key].append(cloud_dirname+files)\n",
    "        else:\n",
    "            cloud_files[key]= [cloud_dirname+files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seviri_final, cloud_final = file_aligner(seviri_files.copy(), cloud_files.copy(), seviri_dirname, cloud_dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aecfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = seviri_final[\"01-12\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ace013-e36a-4361-98eb-b9a652ffecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0,y1,x0,x1 =190,338,673,1030\n",
    " \n",
    "lons_seviri, lats_seviri = read_lon_lat(file)\n",
    "lons_global , lats_global = get_globe_area(0.25).get_lonlats()\n",
    "lons_masked , lats_masked = lons_global[y0:y1,x0:x1] , lats_global[y0:y1,x0:x1]\n",
    "bounds = (lons_masked.min() ,lons_masked.max() , lats_masked.min() , lats_masked.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac23c9-f371-4b42-b5f3-178380e1a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_month = \"12\" # This should be changed based on the desired output of the clustering\n",
    "keys = [key for key in seviri_final.keys() if key.startswith(chosen_month)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fefa04-d55a-4f45-b403-4612d974169f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monthly_hourly_medians = {}\n",
    "for key in keys:\n",
    "    files_list = seviri_final[key]\n",
    "    cloud_files_list = cloud_final[key]\n",
    "    PDI_monthly_hourly_median = get_reference_PDI(files_list , cloud_files_list , lons_seviri, lats_seviri)\n",
    "    monthly_hourly_medians[key] = PDI_monthly_hourly_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e072be-0a00-4f37-b4fe-2c358e844ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seviri_dirname = # the files in the directory are meant to be in chronological order, if this is not the case, rewrite the code in a manner that assures that it is.\n",
    "seviri_files=[]\n",
    "for file in os.listdir(seviri_dirname):\n",
    "    if file.endswith(\".nat\"):\n",
    "        month = file[28:30]\n",
    "        if month == chosen_month:\n",
    "            seviri_files.append( file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530b40d-50b7-499e-b0d7-5183846e35cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PDI_anomalies =[]\n",
    "dates = []\n",
    "\n",
    "for file in seviri_files:\n",
    "    date = file[24:28]+ \"-\"  + file[28:30] + \" \"+ file[30:32]+\":00\"\n",
    "    PDI = read_PDI(seviri_dirname+file, lons_seviri, lats_seviri)\n",
    "    cloud = read_cloud_mask(seviri_dirname+file, lons_seviri, lats_seviri)\n",
    "    PDI_anomaly = get_PDI_anomaly(PDI ,date , monthly_hourly_medians)\n",
    "    PDI_anomaly[cloud]=0\n",
    "    dates.append(date)\n",
    "    PDI_anomalies.append(PDI_anomaly)\n",
    "PDI_anomalies = np.stack(PDI_anomalies)[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b40d33b-8a8f-421b-a363-3cafc2d18470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reshape to to suit the requirments of DBSCAN\n",
    "z = np.indices(PDI_anomalies.shape)[0]\n",
    "x = np.indices(PDI_anomalies.shape)[1]\n",
    "y = np.indices(PDI_anomalies.shape)[2]\n",
    "transformed_time_stack = transformer(PDI_anomalies)\n",
    "features = np.stack([transformed_time_stack, z, x,y], axis=3)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe27fd-d357-41f0-add7-435fd6887144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shape0, shape1, shape2 = features.shape[0] , features.shape[1]  , features.shape[2]\n",
    "features = features.reshape(shape0*shape1*shape2, 4) # (data_points, features)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635bc799-e007-44dc-b81b-96727d28e6fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=1.9, min_samples=4).fit(features)\n",
    "labels = clustering.labels_\n",
    "labels = labels.reshape((shape0, shape1 , shape2)) # Back to spatio-temporal cube\n",
    "labels.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
